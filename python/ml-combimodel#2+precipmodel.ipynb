{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os, warnings\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "#from dask_ml.decomposition import PCA\n",
    "from joblib import dump, load\n",
    "\n",
    "import keras\n",
    "from keras.layers.core import Dropout\n",
    "from keras.models import load_model\n",
    "\n",
    "import geopandas\n",
    "from rasterio import features\n",
    "from affine import Affine\n",
    "\n",
    "import dask\n",
    "#import dask.multiprocessing\n",
    "dask.config.set(scheduler='threads')\n",
    "\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/srvx11/lehre/users/a1254888/.conda/envs/ml_flood/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfloat(f):\n",
    "    return str(float(f))\n",
    "def sint(i):\n",
    "    return str(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glofas_danube():\n",
    "    glofas = xr.open_dataset('../data/danube/glofas_reanalysis_danube_1981-2002.nc')\n",
    "    glofas = glofas.rename({'lat': 'latitude', 'lon': 'longitude'})  # to have the same name like in era5\n",
    "    glofas = shift_time(glofas, -dt.timedelta(days=1))  # the discharge is the mean of the previous 24h of the timestamp\n",
    "    return glofas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def shift_time(ds, value):\n",
    "    ds.coords['time'].values = pd.to_datetime(ds.coords['time'].values) + value\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_riverpoints(dis):\n",
    "    return (dis > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_of_basin(da, kw_basins='Danube'):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "        da : xr.DataArray\n",
    "            contains the coordinates\n",
    "        kw_basins : str\n",
    "            identifier of the basin in the basins dataset\n",
    "    \"\"\"\n",
    "    def transform_from_latlon(lat, lon):\n",
    "        lat = np.asarray(lat)\n",
    "        lon = np.asarray(lon)\n",
    "        trans = Affine.translation(lon[0], lat[0])\n",
    "        scale = Affine.scale(lon[1] - lon[0], lat[1] - lat[0])\n",
    "        return trans * scale\n",
    "\n",
    "    def rasterize(shapes, coords, fill=np.nan, **kwargs):\n",
    "        \"\"\"Rasterize a list of (geometry, fill_value) tuples onto the given\n",
    "        xray coordinates. This only works for 1d latitude and longitude\n",
    "        arrays.\n",
    "        \"\"\"\n",
    "        transform = transform_from_latlon(coords['latitude'], coords['longitude'])\n",
    "        out_shape = (len(coords['latitude']), len(coords['longitude']))\n",
    "        raster = features.rasterize(shapes, out_shape=out_shape,\n",
    "                                    fill=fill, transform=transform,\n",
    "                                    dtype=float, **kwargs)\n",
    "        return xr.DataArray(raster, coords=coords, dims=('latitude', 'longitude'))\n",
    "    \n",
    "    # this shapefile is from natural earth data\n",
    "    # http://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-1-states-provinces/\n",
    "    shp2 = '/raid/home/srvx7/lehre/users/a1303583/ipython/ml_flood/data/drainage_basins/Major_Basins_of_the_World.shp'\n",
    "    basins = geopandas.read_file(shp2)\n",
    "#    print(basins)\n",
    "    single_basin = basins.query(\"NAME == '\"+kw_basins+\"'\").reset_index(drop=True)\n",
    "#    print(single_basin)\n",
    "    shapes = [(shape, n) for n, shape in enumerate(single_basin.geometry)]\n",
    "\n",
    "    da['basins'] = rasterize(shapes, da.coords)\n",
    "    da = da.basins == 0\n",
    "    return da.drop('basins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_upstream(is_river, lat, lon, basin='Danube'):\n",
    "    \n",
    "    \n",
    "    # longitude condition\n",
    "    is_west = (~np.isnan(is_river.where(is_river.longitude <= lon))).astype(bool)\n",
    "    \n",
    "    mask_basin = get_mask_of_basin(is_river, kw_basins=basin)\n",
    "\n",
    "    nearby_mask = is_river*0.\n",
    "    nearby_mask.loc[dict(latitude=slice(lat+1.5, lat-1.5), \n",
    "                         longitude=slice(lon-1.5, lon+1.5))] = 1.\n",
    "    nearby_mask = nearby_mask.astype(bool)\n",
    "    \n",
    "    mask = mask_basin & nearby_mask & is_west #mask_box_mean_greater & \n",
    "    if 'basins' in mask.coords:\n",
    "        mask = mask.drop('basins')\n",
    "    if 'time' in mask.coords:\n",
    "        mask = mask.drop('time')  # time and basins dimension make no sense here\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shifted_predictors(ds, shifts, variables='all'):\n",
    "    \"\"\"Adds additional variables to an array which are shifted in time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "    shifts : list of integers\n",
    "    variables : str or list\n",
    "    \"\"\"\n",
    "    if variables == 'all': \n",
    "        variables = ds.data_vars\n",
    "        \n",
    "    for var in variables:\n",
    "        for i in shifts:\n",
    "            if i == 0: continue  # makes no sense to shift by zero\n",
    "            newvar = var+'-'+str(i)\n",
    "            ds[newvar] = ds[var].shift(time=i)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reshape_flowmodel(X_dis, y_dis):\n",
    "    \"\"\"Reshape, merge predictor/predictand in time, drop nans.\"\"\"\n",
    "    X_dis = X_dis.to_array(dim='time_feature')  \n",
    "    #print('X before feature-stacking', X_dis)\n",
    "    X_dis = X_dis.stack(features=['latitude', 'longitude', 'time_feature'])\n",
    "    #print('X before featuredrop', X_dis)\n",
    "    Xar = X_dis.dropna('features', how='all')\n",
    "    \n",
    "    yar = y_dis\n",
    "    yar = yar.drop(['latitude', 'longitude'])\n",
    "    yar.coords['features'] = 'dis'\n",
    "    \n",
    "    #print('X, y before concat for time nan dropping', Xar, yar)\n",
    "    Xy = xr.concat([Xar, yar], dim='features')\n",
    "    Xyt = Xy.dropna('time', how='any')  # drop them as we cannot train on nan values\n",
    "    time = Xyt.time\n",
    "    \n",
    "    Xda = Xyt[:,:-1]\n",
    "    yda = Xyt[:,-1]\n",
    "    return Xda, yda, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux.floodmodels import FlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = xr.open_dataset('../data/danube/era5_slt_z_slor_lsm_stationary_field.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5 = xr.open_dataset('../data/usa/era5_lsp_cp_1981-2017_daysum.nc')\n",
    "# era5 = shift_time(era5, -dt.timedelta(hours=23))\n",
    "\n",
    "era5 = xr.open_dataset('../data/danube/era5_danube_pressure_and_single_levels.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glofas = read_glofas_danube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glofas = glofas.isel(time=slice(0, 365*15))  # just to reduce the amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tp' in era5:\n",
    "    tp = era5['tp']*1000\n",
    "else:\n",
    "    tp = (era5['cp']+era5['lsp'])*1000\n",
    "tp.name = 'total precip [mm]'\n",
    "tp = tp.interp(latitude=glofas.latitude,\n",
    "               longitude=glofas.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(d):\n",
    "    if not os.path.isdir(d):\n",
    "        os.makedirs(d)\n",
    "        \n",
    "def replace(string: str, old_new: dict):\n",
    "    for o, n in old_new.items(): \n",
    "        string = string.replace(o, str(n))\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare training data for the FlowModel\n",
    "### features = discharge upstream (t-1, ... t-3)\n",
    "### target = discharge (t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = range(1,4)\n",
    "X = add_shifted_predictors(glofas, shifts, variables='all')\n",
    "X = X.drop('dis')  # current dis is to be predicted, is not a feature\n",
    "\n",
    "y = glofas['dis']  # just this variable as dataarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for rgp in riverpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = dict(time=slice(None, '1990'))\n",
    "N_valid = dict(time=slice('1990', '1995'))\n",
    "\n",
    "\n",
    "# kind, lat, lon will be replaced!\n",
    "ff_mod = '../models/flowmodel/danube/kind/point_lat_lon_flowmodel.pkl'\n",
    "ff_hist = '../models/flowmodel/danube/kind/point_lat_lon_history.png'\n",
    "ff_valid = '../models/flowmodel/danube/kind/point_lat_lon_validation.png'\n",
    "ff_upstream = '../models/flowmodel/danube/kind/point_lat_lon_upstream.png'\n",
    "\n",
    "\n",
    "#model = FlowModel('Ridge', dict(alphas=np.logspace(-3, 2, 6)))\n",
    "\n",
    "model = FlowModel('neural_net', dict(epochs=1000, \n",
    "                                     #filepath=filepath,\n",
    "                                      ))\n",
    "pipe = Pipeline([#('scaler', StandardScaler()),\n",
    "                 #('pca', PCA(n_components=6)),\n",
    "                 ('model', model),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.25 8.050000000000011 is spring.\n",
      "48.15 8.050000000000011 is spring.\n",
      "48.05 8.050000000000011 is spring.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "already trained.\n",
      "48.45 10.050000000000011 is danube river -> train flowmodel\n",
      "(802, 282) (802,)\n",
      "(489, 282) (489,)\n",
      "48.35 10.050000000000011 is danube river -> train flowmodel\n",
      "(809, 282) (809,)\n",
      "(489, 282) (489,)\n",
      "48.25 10.050000000000011 is danube river -> train flowmodel\n",
      "(809, 282) (809,)\n",
      "(492, 282) (492,)\n",
      "48.15 10.050000000000011 is danube river -> train flowmodel\n",
      "(810, 282) (810,)\n",
      "(494, 282) (494,)\n",
      "48.05 10.050000000000011 is danube river -> train flowmodel\n",
      "(812, 282) (812,)\n",
      "(495, 282) (495,)\n",
      "47.95 10.050000000000011 is danube river -> train flowmodel\n",
      "(814, 282) (814,)\n",
      "(495, 282) (495,)\n",
      "47.85 10.050000000000011 is danube river -> train flowmodel\n",
      "(822, 282) (822,)\n",
      "(495, 282) (495,)\n",
      "47.75 10.050000000000011 is danube river -> train flowmodel\n",
      "(826, 282) (826,)\n",
      "(501, 282) (501,)\n",
      "47.65 10.050000000000011 is danube river -> train flowmodel\n",
      "(832, 282) (832,)\n",
      "(502, 282) (502,)\n",
      "47.55 10.050000000000011 is danube river -> train flowmodel\n",
      "(832, 282) (832,)\n",
      "(504, 282) (504,)\n",
      "47.45 10.050000000000011 is danube river -> train flowmodel\n",
      "(841, 282) (841,)\n",
      "(505, 282) (505,)\n",
      "47.35 10.050000000000011 is danube river -> train flowmodel\n",
      "(843, 282) (843,)\n",
      "(512, 282) (512,)\n",
      "47.25 10.050000000000011 is danube river -> train flowmodel\n",
      "(850, 282) (850,)\n",
      "(515, 282) (515,)\n"
     ]
    }
   ],
   "source": [
    "#riverpoints = select_riverpoints(glofas)\n",
    "danube_gridpoints = get_mask_of_basin(glofas['dis'].isel(time=0), 'Danube')\n",
    "\n",
    "plt.imshow(danube_gridpoints.astype(int))\n",
    "plt.show()\n",
    "\n",
    "mask_springs = glofas['dis'].isel(time=0)\n",
    "mask_springs.values[:] = 0.\n",
    "\n",
    "dis_map_mean = glofas['dis'].mean('time')\n",
    "is_river = select_riverpoints(dis_map_mean)\n",
    "#river_min_discharge = 20\n",
    "# mask_box_mean_greater = dis_box_mean > river_min_discharge\n",
    "#mask_box_mean_greater = (~np.isnan(dis_map_mean.where(dis_map_mean > river_min_discharge))).astype(bool)\n",
    "plt.imshow(is_river.astype(int))\n",
    "plt.title('is_river')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for lon in danube_gridpoints.longitude:\n",
    "    for lat in danube_gridpoints.latitude:\n",
    "        #print(danube_gridpoints.sel(latitude=lat, longitude=lon))\n",
    "        if danube_gridpoints.sel(latitude=lat, longitude=lon) == 1:\n",
    "            lat, lon = float(lat), float(lon)\n",
    "            lats, lons = sfloat(lat), sfloat(lon)\n",
    "            \n",
    "            f_mod = replace(ff_mod, dict(lat=lat, lon=lon, kind=model.kind))\n",
    "            f_hist = replace(ff_hist, dict(lat=lat, lon=lon, kind=model.kind))\n",
    "            f_valid = replace(ff_valid, dict(lat=lat, lon=lon, kind=model.kind))\n",
    "            f_upstream = replace(ff_upstream, dict(lat=lat, lon=lon, kind=model.kind))\n",
    "            \n",
    "            upstream = select_upstream(is_river, lat, lon, basin='Danube')\n",
    "\n",
    "            N_upstream = int(upstream.sum())\n",
    "            #print(N_upstream)\n",
    "            if N_upstream < 5:\n",
    "                print(lats, lons, 'is spring.')\n",
    "                mask_springs.loc[dict(latitude=lat, longitude=lon)] = 1.\n",
    "\n",
    "                #plt.imshow(mask_springs.astype(int))\n",
    "                #plt.title('springs')\n",
    "                #plt.show()\n",
    "            else:\n",
    "                if os.path.isfile(f_mod):\n",
    "                    print('already trained.')\n",
    "                else:\n",
    "                    print(lats, lons, 'is danube river -> train flowmodel')\n",
    "                    \n",
    "                    try:\n",
    "                        fig, ax = plt.subplots()\n",
    "                        ax.imshow(upstream.astype(int))\n",
    "                        plt.title(str(N_upstream)+' upstream points for '+lats+' '+lons)\n",
    "                        fig.savefig(f_upstream)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    tp_box = tp.loc[dict(latitude=slice(lat+1.5, lat-1.5), \n",
    "                                         longitude=slice(lon-1.5, lon+1.5))] \n",
    "                    noprecip = tp_box.mean(['longitude', 'latitude']) < 0.1\n",
    "\n",
    "                    Xt = X.where(noprecip, drop=True)\n",
    "                    Xt = Xt.where(upstream, drop=True)\n",
    "                    yt = y.sel(latitude=float(lat), longitude=float(lon))\n",
    "                    Xda, yda, time = preprocess_reshape_flowmodel(Xt, yt)\n",
    "                    \n",
    "                    X_train = Xda.loc[N_train] \n",
    "                    y_train = yda.loc[N_train] \n",
    "                    X_valid = Xda.loc[N_valid] \n",
    "                    y_valid = yda.loc[N_valid] \n",
    "                    \n",
    "                    print(X_train.shape, y_train.shape)\n",
    "                    print(X_valid.shape, y_valid.shape)\n",
    "                    ppipe = clone(pipe) \n",
    "                    history = ppipe.fit(X_train.values, y_train.values,\n",
    "                                       model__validation_data=(X_valid.values, \n",
    "                                                               y_valid.values)) \n",
    "                    \n",
    "                    mkdir(os.path.dirname(f_mod))\n",
    "                    dump(ppipe, f_mod)\n",
    "            \n",
    "                    try:\n",
    "                        h = history.named_steps['model'].m.model.history\n",
    "\n",
    "                        # Plot training & validation loss value\n",
    "                        fig, ax = plt.subplots()\n",
    "                        ax.plot(h.history['loss'], label='loss')\n",
    "                        ax.plot(h.history['val_loss'], label='val_loss')\n",
    "                        plt.title('Model loss')\n",
    "                        ax.set_ylabel('Loss')\n",
    "                        ax.set_xlabel('Epoch')\n",
    "                        plt.legend() #['Train', 'Test'], loc='upper left')\n",
    "                        ax.set_yscale('log')\n",
    "                        fig.savefig(f_hist)\n",
    "                    except Exception as e:\n",
    "                        warnings.warn(str(e))\n",
    "\n",
    "                    ppipe = load(f_mod)\n",
    "                    y_m = ppipe.predict(X_valid) \n",
    "                    \n",
    "                    try:\n",
    "                        fig, ax = plt.subplots(figsize=(10,4))\n",
    "                        y_m.to_pandas().plot(ax=ax)\n",
    "                        y_valid.name = 'reanalysis'\n",
    "                        y_valid.to_pandas().plot(ax=ax)\n",
    "                        plt.legend()\n",
    "                        fig.savefig(f_valid)\n",
    "                    except Exception as e:\n",
    "                        warnings.warn(str(e))\n",
    "                    plt.close('all')\n",
    "                    \n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(mask_springs.astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "help(FlowModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def train_flowmodel(lat, lon):    \n",
    "    lats, lons = sfloat(lat), sfloat(lon)\n",
    "\n",
    "    modpath = fm_path.replace('lat', lats).replace('lon', lons)\n",
    "\n",
    "    upstream = select_upstream(is_river, lat, lon, basin='Danube')\n",
    "    N_upstream = int(upstream.sum())\n",
    "    \n",
    "    if N_upstream < 5:\n",
    "        mask_springs.loc[dict(latitude=lat, longitude=lon)] = 1.\n",
    "    else:\n",
    "        if not os.path.isfile(modpath):\n",
    "            print(lats, lons, 'is danube river -> train flowmodel')\n",
    "\n",
    "            tp_box = tp.loc[dict(latitude=slice(lat+1.5, lat-1.5), \n",
    "                                 longitude=slice(lon-1.5, lon+1.5))] \n",
    "            noprecip = tp_box.mean(['longitude', 'latitude']) < 0.1\n",
    "\n",
    "            Xt = X.where(noprecip, drop=True)\n",
    "            Xt = Xt.where(upstream, drop=True)\n",
    "            yt = y.sel(latitude=float(lat), longitude=float(lon))\n",
    "\n",
    "            #print(Xt, upstream)\n",
    "            Xda, yda, time = preprocess_reshape_flowmodel(Xt, yt)\n",
    "            print(Xda.shape, yda.shape)\n",
    "\n",
    "            X_train = Xda.loc[N_train] \n",
    "            y_train = yda.loc[N_train] \n",
    "            X_valid = Xda.loc[N_valid] \n",
    "            y_valid = yda.loc[N_valid] \n",
    "\n",
    "            initialdischarge = np.mean(y_train)\n",
    "            pipe = create_FlowModel(modpath, initialdischarge)\n",
    "\n",
    "            train(pipe, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#riverpoints = select_riverpoints(glofas)\n",
    "danube_gridpoints = get_mask_of_basin(glofas['dis'].isel(time=0), 'Danube')\n",
    "\n",
    "plt.imshow(danube_gridpoints.astype(int))\n",
    "plt.show()\n",
    "\n",
    "mask_springs = glofas['dis'].isel(time=0)\n",
    "mask_springs.values[:] = 0.\n",
    "\n",
    "task_list = []\n",
    "\n",
    "for lon in danube_gridpoints.longitude:\n",
    "    for lat in danube_gridpoints.latitude:\n",
    "        #print(danube_gridpoints.sel(latitude=lat, longitude=lon))\n",
    "        if danube_gridpoints.sel(latitude=lat, longitude=lon) == 1:\n",
    "            task_list.append(train_flowmodel(lat, lon))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    dask.compute(task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24,5))\n",
    "minpred = X_train.min('features')\n",
    "maxpred = X_train.max('features')\n",
    "\n",
    "minpred.plot(ax=ax, linestyle='--', label='predictor-min')\n",
    "maxpred.plot(ax=ax, linestyle='--', label='predictor-max')\n",
    "\n",
    "\n",
    "dis[:,i,j].to_pandas().plot(ax=ax, label='dis-reanalysis')\n",
    "y_train_pred.plot(ax=ax, marker='.', lw=0)\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlim(dt.datetime(1981,1,1), y_train_pred.time.values[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24,5))\n",
    "minpred = add_time(Xda.min(axis=1), time)\n",
    "maxpred = add_time(Xda.max(axis=1), time)\n",
    "\n",
    "minpred.plot(ax=ax, linestyle='--', label='predictor-min')\n",
    "maxpred.plot(ax=ax, linestyle='--', label='predictor-max')\n",
    "\n",
    "dis[:,i,j].to_pandas().plot(ax=ax, label='dis-reanalysis')\n",
    "y_valid_pred.plot(ax=ax, marker='.', lw=0)\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_xlim(y_valid_pred.time.values[0], y_valid_pred.time.values[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Is the NN model better than using the max value of all predictors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_5yr(dis):\n",
    "    return dis/glofas_rl['rl5'].sel(latitude=dis.latitude, longitude=dis.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y_train_pred-y_train)/y_train*100).plot(label=)\n",
    "X_train.max('features').plot(label='max_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
